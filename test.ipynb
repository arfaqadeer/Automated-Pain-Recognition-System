{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346caba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Ecg'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m sensor_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEda_E4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEda_RB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEcg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Keep only the relevant columns\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43msensor_names\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# If ECG is missing (i.e., it's NaN), fill it with zeros\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEcg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEcg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mall():\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Ecg'] not in index\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # for loading scaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the CSV file with the correct separator and decimal\n",
    "df = pd.read_csv(\"dataset/PMCD/PMPDB/raw-data/P01_1/P01_1.csv\", sep=\";\", decimal=\",\")  # shape: (timepoints, 6)\n",
    "\n",
    "# Ensure we only select the columns related to the six sensors\n",
    "sensor_names = [\"Bvp\", \"Eda_E4\", \"Resp\", \"Eda_RB\", \"Ecg\", \"Emg\"]\n",
    "\n",
    "# Keep only the relevant columns\n",
    "df = df[sensor_names]\n",
    "\n",
    "# If ECG is missing (i.e., it's NaN), fill it with zeros\n",
    "if 'Ecg' not in df.columns or df['Ecg'].isnull().all():\n",
    "    df['Ecg'] = 0  # Fill all missing ECG values with zeros\n",
    "\n",
    "# Ensure it's a numpy array\n",
    "data = df.to_numpy()  # shape: (timepoints, 6)\n",
    "\n",
    "# Reshape to match your model input: (1 sample, 6 sensors, timepoints)\n",
    "data = data.T[np.newaxis, ...]  # shape: (1, 6, timepoints)\n",
    "\n",
    "# Now you can proceed with the rest of your prediction pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6300d08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1186326 features, but SelectKBest is expecting 162 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# === Transform features ===\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m selected_feat \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m normalized_feat \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(selected_feat)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# === Load model and predict ===\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:107\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m preserve_X \u001b[38;5;241m=\u001b[39m output_config_dense \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_pandas_df(X)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# note: we use get_tags instead of __sklearn_tags__ because this is a\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_check_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1186326 features, but SelectKBest is expecting 162 features as input."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Load your CSV file ===\n",
    "df = pd.read_csv(\"dataset/PMCD/PMPDB/raw-data/P01_1/P01_1.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "# === Ensure only 6 known sensor columns ===\n",
    "sensor_names = [\"Bvp\", \"Eda_E4\", \"Resp\", \"Eda_RB\", \"Ecg\", \"Emg\"]\n",
    "df = df[[col for col in sensor_names if col in df.columns]]\n",
    "\n",
    "# Fill missing columns (e.g., ECG) with zeros\n",
    "for sensor in sensor_names:\n",
    "    if sensor not in df.columns:\n",
    "        df[sensor] = 0\n",
    "\n",
    "# === Convert to array ===\n",
    "data = df[sensor_names].to_numpy().T[np.newaxis, ...]  # shape: (1, 6, timepoints)\n",
    "\n",
    "# === Feature extraction ===\n",
    "def extract_time_features(sample):\n",
    "    features = []\n",
    "    for channel in sample.T:\n",
    "        features.extend([\n",
    "            np.mean(channel), np.std(channel), np.var(channel),\n",
    "            np.min(channel), np.max(channel), np.ptp(channel),\n",
    "            np.percentile(channel, 25), np.percentile(channel, 50), np.percentile(channel, 75),\n",
    "            skew(channel, nan_policy=\"omit\"), kurtosis(channel, nan_policy=\"omit\"),\n",
    "            entropy(np.histogram(channel, bins=10)[0] + 1),\n",
    "            np.sum(channel), np.sqrt(np.mean(channel ** 2)),\n",
    "            np.mean(np.abs(channel)), np.mean(np.diff(channel)),\n",
    "            np.std(np.diff(channel)), np.min(np.diff(channel)),\n",
    "            np.max(np.diff(channel)), np.mean(np.abs(np.diff(channel)))\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_freq_features(sample, sample_rate=1.0):\n",
    "    features = []\n",
    "    for channel in sample.T:\n",
    "        N = len(channel)\n",
    "        freqs = fftfreq(N, 1 / sample_rate)\n",
    "        fft_vals = np.abs(fft(channel))\n",
    "        features.extend([\n",
    "            np.sum(fft_vals), np.mean(fft_vals), np.std(fft_vals),\n",
    "            np.max(fft_vals), freqs[np.argmax(fft_vals)],\n",
    "            np.sum(freqs * fft_vals) / np.sum(fft_vals),\n",
    "            np.sum((freqs ** 2) * fft_vals) / np.sum(fft_vals),\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "# Apply extraction\n",
    "time_feat = extract_time_features(data[0])  # shape: (120,)\n",
    "freq_feat = extract_freq_features(data[0])  # shape: (42,)\n",
    "combined_feat = np.hstack((time_feat, freq_feat)).reshape(1, -1)  # shape: (1, 162)\n",
    "\n",
    "# === Load feature selector and scaler ===\n",
    "selector = joblib.load(\"feature_selector.pkl\")  # expects 162 input features\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# === Transform features ===\n",
    "selected_feat = selector.transform(combined_feat)\n",
    "normalized_feat = scaler.transform(selected_feat)\n",
    "\n",
    "# === Load model and predict ===\n",
    "model = load_model(\"pain_model.h5\")\n",
    "pred = model.predict(normalized_feat)\n",
    "pain_level = np.argmax(pred)\n",
    "\n",
    "print(f\"Predicted Pain Level: {pain_level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc00c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the data: ['Seconds', 'Bvp', 'Eda_E4', 'Tmp', 'Resp', 'Eda_RB', 'Bvp_RB', 'Emg', 'Grip', 'Pain rates', 'Pain labels']\n",
      "Warning: 'Ecg' column is missing from the data.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # for loading scaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the CSV file with the correct separator and decimal\n",
    "df = pd.read_csv(\"dataset/PMCD/PMPDB/raw-data/P01_1/P01_1.csv\", sep=\";\", decimal=\",\")  # shape: (timepoints, 6)\n",
    "\n",
    "# Print the column names to check what they are\n",
    "print(f\"Columns in the data: {df.columns.tolist()}\")\n",
    "\n",
    "# Ensure we only select the columns related to the six sensors\n",
    "sensor_names = [\"Bvp\", \"Eda_E4\", \"Resp\", \"Eda_RB\", \"Ecg\", \"Emg\"]\n",
    "\n",
    "# Check for missing columns and add them as needed\n",
    "for sensor in sensor_names:\n",
    "    if sensor not in df.columns:\n",
    "        print(f\"Warning: '{sensor}' column is missing from the data.\")\n",
    "        df[sensor] = 0  # Fill missing sensor data with zeros\n",
    "\n",
    "# Now, keep only the relevant columns\n",
    "df = df[sensor_names]\n",
    "\n",
    "# If ECG is missing (i.e., it's NaN), fill it with zeros\n",
    "if 'Ecg' not in df.columns or df['Ecg'].isnull().all():\n",
    "    df['Ecg'] = 0  # Fill all missing ECG values with zeros\n",
    "\n",
    "# Ensure it's a numpy array\n",
    "data = df.to_numpy()  # shape: (timepoints, 6)\n",
    "\n",
    "# Reshape to match your model input: (1 sample, 6 sensors, timepoints)\n",
    "data = data.T[np.newaxis, ...]  # shape: (1, 6, timepoints)\n",
    "\n",
    "# Now you can proceed with the rest of your prediction pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(sample):\n",
    "    features = []\n",
    "    for channel in sample.T:\n",
    "        features.extend([\n",
    "            np.mean(channel), np.std(channel), np.var(channel),\n",
    "            np.min(channel), np.max(channel), np.ptp(channel),\n",
    "            np.percentile(channel, 25), np.percentile(channel, 50), np.percentile(channel, 75),\n",
    "            skew(channel, nan_policy=\"omit\"), kurtosis(channel, nan_policy=\"omit\"),\n",
    "            entropy(np.histogram(channel, bins=10)[0] + 1),\n",
    "            np.sum(channel), np.sqrt(np.mean(channel ** 2)),\n",
    "            np.mean(np.abs(channel)), np.mean(np.diff(channel)),\n",
    "            np.std(np.diff(channel)), np.min(np.diff(channel)),\n",
    "            np.max(np.diff(channel)), np.mean(np.abs(np.diff(channel)))\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_freq_features(sample, sample_rate=1.0):\n",
    "    features = []\n",
    "    for channel in sample.T:\n",
    "        N = len(channel)\n",
    "        freqs = fftfreq(N, 1 / sample_rate)\n",
    "        fft_vals = np.abs(fft(channel))\n",
    "        features.extend([\n",
    "            np.sum(fft_vals), np.mean(fft_vals), np.std(fft_vals),\n",
    "            np.max(fft_vals), freqs[np.argmax(fft_vals)],\n",
    "            np.sum(freqs * fft_vals) / np.sum(fft_vals),\n",
    "            np.sum((freqs ** 2) * fft_vals) / np.sum(fft_vals),\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features\n",
    "time_feat = extract_time_features(data[0])  # shape: (6×20,)\n",
    "freq_feat = extract_freq_features(data[0])  # shape: (6×7,)\n",
    "\n",
    "combined_feat = np.hstack((time_feat, freq_feat)).reshape(1, -1)  # shape: (1, total_features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb86866",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1186326 features, but SelectKBest is expecting 162 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the selector you used for training (if saved), or re-apply the selection logic\u001b[39;00m\n\u001b[0;32m      6\u001b[0m selector \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_selector.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Assuming you saved it\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m selected_feat \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_feat\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: (1, 100)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:107\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m preserve_X \u001b[38;5;241m=\u001b[39m output_config_dense \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_pandas_df(X)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# note: we use get_tags instead of __sklearn_tags__ because this is a\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_check_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1186326 features, but SelectKBest is expecting 162 features as input."
     ]
    }
   ],
   "source": [
    "# Handle missing values (same as training)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "combined_feat = imputer.fit_transform(combined_feat)\n",
    "\n",
    "# Load the selector you used for training (if saved), or re-apply the selection logic\n",
    "selector = joblib.load(\"feature_selector.pkl\")  # Assuming you saved it\n",
    "selected_feat = selector.transform(combined_feat)  # shape: (1, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d46b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "normalized_feat = scaler.transform(selected_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226afc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"pain_model.h5\")\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(normalized_feat)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"Predicted Pain Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c900368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV correctly\n",
    "df = pd.read_csv(\"dataset/PMCD/PMPDB/raw-data/P01_1/P01_1.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "# Keep only these 6 sensors\n",
    "sensor_names = [\"Bvp\", \"Eda_E4\", \"Resp\", \"Eda_RB\", \"Ecg\", \"Emg\"]\n",
    "df = df[[col for col in sensor_names if col in df.columns]]\n",
    "\n",
    "# If any sensor is missing, add it and fill with zeros\n",
    "for sensor in sensor_names:\n",
    "    if sensor not in df.columns:\n",
    "        df[sensor] = 0.0\n",
    "\n",
    "# Ensure order is correct\n",
    "df = df[sensor_names]\n",
    "\n",
    "# Convert to numpy format and reshape\n",
    "data = df.to_numpy().T[np.newaxis, ...]  # shape: (1, 6, timepoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d05bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1, 1186326)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "def extract_time_features(sample):\n",
    "    features = []\n",
    "    for channel in sample.T:\n",
    "        features.extend([\n",
    "            np.mean(channel), np.std(channel), np.var(channel),\n",
    "            np.min(channel), np.max(channel), np.ptp(channel),\n",
    "            np.percentile(channel, 25), np.percentile(channel, 50), np.percentile(channel, 75),\n",
    "            skew(channel, nan_policy=\"omit\"), kurtosis(channel, nan_policy=\"omit\"),\n",
    "            entropy(np.histogram(channel, bins=10)[0] + 1),\n",
    "            np.sum(channel), np.sqrt(np.mean(channel ** 2)),\n",
    "            np.mean(np.abs(channel)), np.mean(np.diff(channel)),\n",
    "            np.std(np.diff(channel)), np.min(np.diff(channel)),\n",
    "            np.max(np.diff(channel)), np.mean(np.abs(np.diff(channel)))\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_freq_features(sample, sample_rate=1.0):\n",
    "    features = []\n",
    "    for channel in sample.T:\n",
    "        N = len(channel)\n",
    "        freqs = fftfreq(N, 1 / sample_rate)\n",
    "        fft_vals = np.abs(fft(channel))\n",
    "        features.extend([\n",
    "            np.sum(fft_vals), np.mean(fft_vals), np.std(fft_vals),\n",
    "            np.max(fft_vals), freqs[np.argmax(fft_vals)],\n",
    "            np.sum(freqs * fft_vals) / np.sum(fft_vals),\n",
    "            np.sum((freqs ** 2) * fft_vals) / np.sum(fft_vals),\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features\n",
    "time_feat = extract_time_features(data[0])\n",
    "freq_feat = extract_freq_features(data[0])\n",
    "combined_feat = np.hstack((time_feat, freq_feat)).reshape(1, -1)\n",
    "print(f\"X_train shape: {combined_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "babc7b4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1186326 features, but SelectKBest is expecting 162 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Apply same transformations\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m selected_feat \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m normalized_feat \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(selected_feat)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:107\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m preserve_X \u001b[38;5;241m=\u001b[39m output_config_dense \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_pandas_df(X)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# note: we use get_tags instead of __sklearn_tags__ because this is a\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_check_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1186326 features, but SelectKBest is expecting 162 features as input."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load selector and scaler\n",
    "selector = joblib.load(\"feature_selector.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Apply same transformations\n",
    "selected_feat = selector.transform(combined_feat)\n",
    "normalized_feat = scaler.transform(selected_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1715e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"pain_model.h5\")\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(normalized_feat)\n",
    "predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "print(\"Predicted Pain Level:\", predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
